{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KRT Diabetes Knowledge Evaluation\n",
    "\n",
    "This notebook evaluates the impact of injecting curated Type 2 Diabetes Q&A (Key Risk Topic approach) into the TrustMed AI system.\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- Answer accuracy vs ground truth\n",
    "- Before/after comparison (baseline vs KRT-enhanced)\n",
    "- Key term coverage\n",
    "- Factual correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import project modules\n",
    "from krt_diabetes_knowledge import TOP_TYPE2_QA, format_top_qa_text\n",
    "from creat_graph_ollama import call_ollama, get_ollama_embedding\n",
    "from camel.storages import Neo4jGraph\n",
    "\n",
    "print(f\"Loaded {len(TOP_TYPE2_QA)} curated diabetes Q&A pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Neo4j connection\n",
    "neo4j_url = os.getenv(\"NEO4J_URL\", \"bolt://localhost:7687\")\n",
    "neo4j_user = os.getenv(\"NEO4J_USERNAME\", \"neo4j\")\n",
    "neo4j_password = os.getenv(\"NEO4J_PASSWORD\", \"password\")\n",
    "\n",
    "n4j = Neo4jGraph(\n",
    "    url=neo4j_url,\n",
    "    username=neo4j_user,\n",
    "    password=neo4j_password\n",
    ")\n",
    "\n",
    "print(f\"Connected to Neo4j at {neo4j_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_baseline_response(question, model=\"llama3\"):\n",
    "    \"\"\"\n",
    "    Generate response WITHOUT KRT context injection (baseline).\n",
    "    Uses only the model's inherent knowledge.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are a medical assistant providing information about diabetes.\n",
    "Answer the question concisely and accurately based on your medical knowledge.\n",
    "Keep your answer to 2-3 paragraphs.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"Question: {question}\"\n",
    "    \n",
    "    response = call_ollama(system_prompt + \"\\n\\n\" + user_prompt, model)\n",
    "    return response\n",
    "\n",
    "\n",
    "def generate_krt_enhanced_response(question, model=\"llama3\"):\n",
    "    \"\"\"\n",
    "    Generate response WITH KRT context injection (enhanced).\n",
    "    Injects curated diabetes knowledge as reference.\n",
    "    \"\"\"\n",
    "    krt_context = format_top_qa_text()\n",
    "    \n",
    "    system_prompt = \"\"\"You are a medical assistant providing information about diabetes.\n",
    "Answer the question concisely and accurately.\n",
    "You have access to curated reference material below - use it to provide accurate information.\n",
    "Keep your answer to 2-3 paragraphs.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Reference Material:\n",
    "======================================================================\n",
    "** REFERENCE: CURATED TYPE 2 DIABETES KNOWLEDGE (KRT) **\n",
    "{krt_context}\n",
    "======================================================================\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "    \n",
    "    response = call_ollama(system_prompt + \"\\n\\n\" + user_prompt, model)\n",
    "    return response\n",
    "\n",
    "\n",
    "print(\"Response generation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_terms(text):\n",
    "    \"\"\"\n",
    "    Extract key medical/factual terms from text.\n",
    "    Returns set of lowercase terms.\n",
    "    \"\"\"\n",
    "    # Medical and factual keywords to look for\n",
    "    patterns = [\n",
    "        r'\\b\\d+(?:\\.\\d+)?\\s*(?:million|percent|%|mg|years?|days?)\\b',  # Numbers with units\n",
    "        r'\\btype\\s*[12]\\s*diabetes\\b',\n",
    "        r'\\b(?:CDC|NIDDK|NIH|WHO)\\b',  # Organizations\n",
    "        r'\\binsulin\\s*resistance\\b',\n",
    "        r'\\bblood\\s*(?:sugar|glucose)\\b',\n",
    "        r'\\bA1C\\b',\n",
    "        r'\\b(?:hyper|hypo)glycemia\\b',\n",
    "        r'\\bprediabetes\\b',\n",
    "        r'\\b(?:heart|kidney|liver)\\s*disease\\b',\n",
    "        r'\\bneuropathy\\b',\n",
    "        r'\\b(?:GLP-1|SGLT2)\\b',\n",
    "        r'\\bmetformin\\b',\n",
    "        r'\\bbeta\\s*cells?\\b',\n",
    "        r'\\bpancreas\\b',\n",
    "    ]\n",
    "    \n",
    "    terms = set()\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text_lower, re.IGNORECASE)\n",
    "        terms.update([m.strip().lower() for m in matches])\n",
    "    \n",
    "    return terms\n",
    "\n",
    "\n",
    "def calculate_key_term_overlap(response, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate percentage of key terms from ground truth present in response.\n",
    "    \"\"\"\n",
    "    ground_truth_terms = extract_key_terms(ground_truth)\n",
    "    response_terms = extract_key_terms(response)\n",
    "    \n",
    "    if not ground_truth_terms:\n",
    "        return 0.0\n",
    "    \n",
    "    overlap = ground_truth_terms.intersection(response_terms)\n",
    "    overlap_percentage = len(overlap) / len(ground_truth_terms) * 100\n",
    "    \n",
    "    return overlap_percentage\n",
    "\n",
    "\n",
    "def check_source_citation(response):\n",
    "    \"\"\"\n",
    "    Check if response mentions authoritative sources.\n",
    "    \"\"\"\n",
    "    sources = ['CDC', 'NIDDK', 'NIH', 'WHO', 'American Diabetes Association']\n",
    "    cited_sources = []\n",
    "    \n",
    "    for source in sources:\n",
    "        if source.lower() in response.lower():\n",
    "            cited_sources.append(source)\n",
    "    \n",
    "    return cited_sources\n",
    "\n",
    "\n",
    "def check_critical_facts(response, ground_truth):\n",
    "    \"\"\"\n",
    "    Check for presence of critical facts from ground truth.\n",
    "    Returns dict with fact checks.\n",
    "    \"\"\"\n",
    "    facts_found = {}\n",
    "    \n",
    "    # Extract numbers from ground truth\n",
    "    numbers = re.findall(r'\\b(\\d+(?:\\.\\d+)?)\\s*(?:million|percent|%)', ground_truth)\n",
    "    \n",
    "    for num in numbers:\n",
    "        facts_found[f\"mentions_{num}\"] = num in response\n",
    "    \n",
    "    # Check for key concepts\n",
    "    key_concepts = [\n",
    "        ('insulin_resistance', r'insulin\\s*resistance'),\n",
    "        ('autoimmune', r'autoimmune'),\n",
    "        ('lifestyle', r'lifestyle|diet|exercise|weight'),\n",
    "        ('complications', r'complication|heart|kidney|nerve|eye'),\n",
    "    ]\n",
    "    \n",
    "    for concept_name, pattern in key_concepts:\n",
    "        if re.search(pattern, ground_truth, re.IGNORECASE):\n",
    "            facts_found[concept_name] = bool(re.search(pattern, response, re.IGNORECASE))\n",
    "    \n",
    "    return facts_found\n",
    "\n",
    "\n",
    "print(\"Evaluation metrics functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Evaluation: Baseline vs KRT-Enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "evaluation_results = []\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, qa_pair in enumerate(TOP_TYPE2_QA):\n",
    "    question = qa_pair['question']\n",
    "    ground_truth = qa_pair['answer']\n",
    "    krt_focus = qa_pair['krt_focus']\n",
    "    \n",
    "    print(f\"\\nQ{i+1}: {question[:60]}...\")\n",
    "    \n",
    "    # Generate baseline response (without KRT)\n",
    "    print(\"  Generating baseline response...\")\n",
    "    baseline_response = generate_baseline_response(question)\n",
    "    \n",
    "    # Generate KRT-enhanced response\n",
    "    print(\"  Generating KRT-enhanced response...\")\n",
    "    krt_response = generate_krt_enhanced_response(question)\n",
    "    \n",
    "    # Calculate metrics for baseline\n",
    "    baseline_overlap = calculate_key_term_overlap(baseline_response, ground_truth)\n",
    "    baseline_sources = check_source_citation(baseline_response)\n",
    "    baseline_facts = check_critical_facts(baseline_response, ground_truth)\n",
    "    baseline_fact_score = sum(baseline_facts.values()) / max(len(baseline_facts), 1) * 100\n",
    "    \n",
    "    # Calculate metrics for KRT-enhanced\n",
    "    krt_overlap = calculate_key_term_overlap(krt_response, ground_truth)\n",
    "    krt_sources = check_source_citation(krt_response)\n",
    "    krt_facts = check_critical_facts(krt_response, ground_truth)\n",
    "    krt_fact_score = sum(krt_facts.values()) / max(len(krt_facts), 1) * 100\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'question_id': qa_pair['id'],\n",
    "        'question': question,\n",
    "        'krt_focus': krt_focus,\n",
    "        'ground_truth': ground_truth,\n",
    "        'baseline_response': baseline_response,\n",
    "        'krt_response': krt_response,\n",
    "        'baseline_key_term_overlap': baseline_overlap,\n",
    "        'krt_key_term_overlap': krt_overlap,\n",
    "        'baseline_sources': baseline_sources,\n",
    "        'krt_sources': krt_sources,\n",
    "        'baseline_fact_score': baseline_fact_score,\n",
    "        'krt_fact_score': krt_fact_score,\n",
    "    }\n",
    "    evaluation_results.append(result)\n",
    "    \n",
    "    print(f\"  Baseline: {baseline_overlap:.1f}% key terms, {baseline_fact_score:.1f}% facts\")\n",
    "    print(f\"  KRT-Enhanced: {krt_overlap:.1f}% key terms, {krt_fact_score:.1f}% facts\")\n",
    "    print(f\"  Improvement: +{krt_overlap - baseline_overlap:.1f}% terms, +{krt_fact_score - baseline_fact_score:.1f}% facts\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "avg_baseline_terms = results_df['baseline_key_term_overlap'].mean()\n",
    "avg_krt_terms = results_df['krt_key_term_overlap'].mean()\n",
    "avg_baseline_facts = results_df['baseline_fact_score'].mean()\n",
    "avg_krt_facts = results_df['krt_fact_score'].mean()\n",
    "\n",
    "print(f\"\\nKey Term Overlap (avg):\")\n",
    "print(f\"  Baseline: {avg_baseline_terms:.1f}%\")\n",
    "print(f\"  KRT-Enhanced: {avg_krt_terms:.1f}%\")\n",
    "print(f\"  Improvement: +{avg_krt_terms - avg_baseline_terms:.1f}%\")\n",
    "\n",
    "print(f\"\\nFactual Accuracy (avg):\")\n",
    "print(f\"  Baseline: {avg_baseline_facts:.1f}%\")\n",
    "print(f\"  KRT-Enhanced: {avg_krt_facts:.1f}%\")\n",
    "print(f\"  Improvement: +{avg_krt_facts - avg_baseline_facts:.1f}%\")\n",
    "\n",
    "# Source citations\n",
    "baseline_with_sources = sum(1 for r in evaluation_results if r['baseline_sources'])\n",
    "krt_with_sources = sum(1 for r in evaluation_results if r['krt_sources'])\n",
    "\n",
    "print(f\"\\nSource Citations:\")\n",
    "print(f\"  Baseline: {baseline_with_sources}/{len(evaluation_results)} responses cite sources\")\n",
    "print(f\"  KRT-Enhanced: {krt_with_sources}/{len(evaluation_results)} responses cite sources\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Bar chart comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Key Term Overlap\n",
    "question_numbers = [f\"Q{i+1}\" for i in range(len(evaluation_results))]\n",
    "x_positions = np.arange(len(question_numbers))\n",
    "bar_width = 0.35\n",
    "\n",
    "baseline_terms = [r['baseline_key_term_overlap'] for r in evaluation_results]\n",
    "krt_terms = [r['krt_key_term_overlap'] for r in evaluation_results]\n",
    "\n",
    "axes[0].bar(x_positions - bar_width/2, baseline_terms, bar_width, label='Baseline', color='#ff7f7f')\n",
    "axes[0].bar(x_positions + bar_width/2, krt_terms, bar_width, label='KRT-Enhanced', color='#7fbf7f')\n",
    "axes[0].set_xlabel('Question')\n",
    "axes[0].set_ylabel('Key Term Overlap (%)')\n",
    "axes[0].set_title('Key Term Coverage: Baseline vs KRT-Enhanced')\n",
    "axes[0].set_xticks(x_positions)\n",
    "axes[0].set_xticklabels(question_numbers)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Factual Accuracy\n",
    "baseline_facts_scores = [r['baseline_fact_score'] for r in evaluation_results]\n",
    "krt_facts_scores = [r['krt_fact_score'] for r in evaluation_results]\n",
    "\n",
    "axes[1].bar(x_positions - bar_width/2, baseline_facts_scores, bar_width, label='Baseline', color='#ff7f7f')\n",
    "axes[1].bar(x_positions + bar_width/2, krt_facts_scores, bar_width, label='KRT-Enhanced', color='#7fbf7f')\n",
    "axes[1].set_xlabel('Question')\n",
    "axes[1].set_ylabel('Factual Accuracy Score (%)')\n",
    "axes[1].set_title('Factual Accuracy: Baseline vs KRT-Enhanced')\n",
    "axes[1].set_xticks(x_positions)\n",
    "axes[1].set_xticklabels(question_numbers)\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('KRT Diabetes Knowledge Integration - Performance Evaluation', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('krt_evaluation_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'krt_evaluation_results.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvement distribution\n",
    "improvements = [\n",
    "    r['krt_key_term_overlap'] - r['baseline_key_term_overlap'] \n",
    "    for r in evaluation_results\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "ax.bar(question_numbers, improvements, color=colors, alpha=0.7)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_xlabel('Question')\n",
    "ax.set_ylabel('Improvement in Key Term Overlap (%)')\n",
    "ax.set_title('Key Term Coverage Improvement per Question')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add average line\n",
    "avg_improvement = np.mean(improvements)\n",
    "ax.axhline(y=avg_improvement, color='blue', linestyle='--', linewidth=2, \n",
    "           label=f'Average: +{avg_improvement:.1f}%')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Response Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comparison(result_index):\n",
    "    \"\"\"\n",
    "    Display side-by-side comparison for a specific question.\n",
    "    \"\"\"\n",
    "    result = evaluation_results[result_index]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"QUESTION {result['question_id']}: {result['question']}\")\n",
    "    print(f\"KRT Focus: {result['krt_focus']}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nüìä GROUND TRUTH (Expected Answer):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result['ground_truth'])\n",
    "    \n",
    "    print(\"\\n\\nüî¥ BASELINE RESPONSE (Without KRT):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result['baseline_response'])\n",
    "    print(f\"\\n  Key Term Overlap: {result['baseline_key_term_overlap']:.1f}%\")\n",
    "    print(f\"  Factual Score: {result['baseline_fact_score']:.1f}%\")\n",
    "    print(f\"  Sources Cited: {result['baseline_sources'] if result['baseline_sources'] else 'None'}\")\n",
    "    \n",
    "    print(\"\\n\\nüü¢ KRT-ENHANCED RESPONSE (With KRT):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result['krt_response'])\n",
    "    print(f\"\\n  Key Term Overlap: {result['krt_key_term_overlap']:.1f}%\")\n",
    "    print(f\"  Factual Score: {result['krt_fact_score']:.1f}%\")\n",
    "    print(f\"  Sources Cited: {result['krt_sources'] if result['krt_sources'] else 'None'}\")\n",
    "    \n",
    "    improvement_terms = result['krt_key_term_overlap'] - result['baseline_key_term_overlap']\n",
    "    improvement_facts = result['krt_fact_score'] - result['baseline_fact_score']\n",
    "    \n",
    "    print(\"\\n\\nüìà IMPROVEMENT:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Key Term Overlap: {'+' if improvement_terms >= 0 else ''}{improvement_terms:.1f}%\")\n",
    "    print(f\"  Factual Accuracy: {'+' if improvement_facts >= 0 else ''}{improvement_facts:.1f}%\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# Display first comparison as example\n",
    "print(\"Example comparison for Question 1:\\n\")\n",
    "display_comparison(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive: View any question comparison\n",
    "# Change the index (0-9) to view different questions\n",
    "question_to_view = 3  # Change this to view different questions (0-9)\n",
    "display_comparison(question_to_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Metrics Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final report\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"        KRT DIABETES KNOWLEDGE INTEGRATION - FINAL REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä OVERALL PERFORMANCE METRICS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Key metrics table\n",
    "metrics_table = pd.DataFrame({\n",
    "    'Metric': ['Key Term Overlap', 'Factual Accuracy', 'Source Citations'],\n",
    "    'Baseline': [\n",
    "        f\"{avg_baseline_terms:.1f}%\",\n",
    "        f\"{avg_baseline_facts:.1f}%\",\n",
    "        f\"{baseline_with_sources}/{len(evaluation_results)}\"\n",
    "    ],\n",
    "    'KRT-Enhanced': [\n",
    "        f\"{avg_krt_terms:.1f}%\",\n",
    "        f\"{avg_krt_facts:.1f}%\",\n",
    "        f\"{krt_with_sources}/{len(evaluation_results)}\"\n",
    "    ],\n",
    "    'Improvement': [\n",
    "        f\"+{avg_krt_terms - avg_baseline_terms:.1f}%\",\n",
    "        f\"+{avg_krt_facts - avg_baseline_facts:.1f}%\",\n",
    "        f\"+{krt_with_sources - baseline_with_sources}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(metrics_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nüìã PER-QUESTION BREAKDOWN\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "question_summary = pd.DataFrame({\n",
    "    'Q#': [f\"Q{r['question_id']}\" for r in evaluation_results],\n",
    "    'KRT Focus': [r['krt_focus'][:40] + '...' for r in evaluation_results],\n",
    "    'Baseline': [f\"{r['baseline_key_term_overlap']:.0f}%\" for r in evaluation_results],\n",
    "    'KRT': [f\"{r['krt_key_term_overlap']:.0f}%\" for r in evaluation_results],\n",
    "    'Œî': [f\"+{r['krt_key_term_overlap'] - r['baseline_key_term_overlap']:.0f}%\" for r in evaluation_results],\n",
    "    'Pass': ['‚úÖ' if r['krt_key_term_overlap'] > r['baseline_key_term_overlap'] else '‚ùå' for r in evaluation_results]\n",
    "})\n",
    "\n",
    "print(question_summary.to_string(index=False))\n",
    "\n",
    "# Pass rate\n",
    "pass_count = sum(1 for r in evaluation_results if r['krt_key_term_overlap'] > r['baseline_key_term_overlap'])\n",
    "pass_rate = pass_count / len(evaluation_results) * 100\n",
    "\n",
    "print(f\"\\n\\n‚úÖ PASS RATE: {pass_count}/{len(evaluation_results)} ({pass_rate:.0f}%)\")\n",
    "print(f\"   Questions where KRT injection improved key term coverage\")\n",
    "\n",
    "print(\"\\n\\nüéØ CONCLUSION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "if avg_krt_terms > avg_baseline_terms:\n",
    "    print(f\"‚úÖ KRT diabetes knowledge injection IMPROVES answer quality\")\n",
    "    print(f\"   - Average key term overlap increased by {avg_krt_terms - avg_baseline_terms:.1f}%\")\n",
    "    print(f\"   - Average factual accuracy improved by {avg_krt_facts - avg_baseline_facts:.1f}%\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Results inconclusive - further tuning may be needed\")\n",
    "\n",
    "print(f\"\\n   Recommendation: The KRT context injection provides the model with\")\n",
    "print(f\"   authoritative, source-cited information that results in more accurate\")\n",
    "print(f\"   and comprehensive responses to diabetes-related questions.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV for further analysis\n",
    "results_df.to_csv('krt_evaluation_results.csv', index=False)\n",
    "print(\"Results saved to 'krt_evaluation_results.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
